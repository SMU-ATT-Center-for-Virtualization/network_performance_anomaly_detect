{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://keras.io/examples/timeseries/timeseries_transformer_classification/\n",
    "logger_name = \"lstnet\"\n",
    "\n",
    "# Path appended in order to import from util\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from util.model_util import LoadModel, SaveModel, SaveResults, SaveHistory\n",
    "from util.Msglog import LogInit\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "#from lstnet_util import GetArguments, LSTNetInit\n",
    "from pandas_util import DataUtil\n",
    "#from lstnet_model import PreSkipTrans, PostSkipTrans, PreARTrans, PostARTrans, LSTNetModel, ModelCompile\n",
    "#from lstnet_plot import AutoCorrelationPlot, PlotHistory, PlotPrediction\n",
    "\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from tensorflow.keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3357: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,14,20,77,78,79,80,81,82) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9726.925995055048\n",
      "6670.752950995366\n"
     ]
    }
   ],
   "source": [
    "# Reading data\n",
    "#filename = 'bq_results_8_12_2020.csv'\\\n",
    "filename = '..\\\\data\\\\bq_results_8_12_2020.csv'\n",
    "trainpercent = 0.6\n",
    "validpercent = 0.2\n",
    "horizon=0\n",
    "window=10\n",
    "normalize=0\n",
    "Data = DataUtil(filename,\n",
    "                trainpercent,\n",
    "                validpercent,\n",
    "                horizon,\n",
    "                window,\n",
    "                normalise=normalize,\n",
    "                dependent_variable=\"iperf_throughput_1_thread\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.43849233 -0.5917273  -0.6500752 ]\n",
      "  [ 0.14812031 -0.5917407  -0.6500752 ]\n",
      "  [ 0.04063619 -0.5921704  -0.6500752 ]\n",
      "  ...\n",
      "  [ 0.16401057 -0.59190184 -0.6500752 ]\n",
      "  [ 0.13357922 -0.5921704  -0.6500752 ]\n",
      "  [ 0.26369947 -0.5920361  -0.6500752 ]]\n",
      "\n",
      " [[ 0.14812031 -0.5917407  -0.6500752 ]\n",
      "  [ 0.04063619 -0.5921704  -0.6500752 ]\n",
      "  [ 0.19519146 -0.5918884  -0.6500752 ]\n",
      "  ...\n",
      "  [ 0.13357922 -0.5921704  -0.6500752 ]\n",
      "  [ 0.26369947 -0.5920361  -0.6500752 ]\n",
      "  [-0.33908105 -0.5916736  -0.6500752 ]]\n",
      "\n",
      " [[ 0.04063619 -0.5921704  -0.6500752 ]\n",
      "  [ 0.19519146 -0.5918884  -0.6500752 ]\n",
      "  [-0.1513961  -0.59145874 -0.6500752 ]\n",
      "  ...\n",
      "  [ 0.26369947 -0.5920361  -0.6500752 ]\n",
      "  [-0.33908105 -0.5916736  -0.6500752 ]\n",
      "  [ 0.31586748 -0.59214354 -0.6500752 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.778334   -0.5929089  -0.6500752 ]\n",
      "  [ 1.2208627  -0.5928015  -0.6500752 ]\n",
      "  [ 2.67722    -0.59336543 -0.6500752 ]\n",
      "  ...\n",
      "  [ 1.408098   -0.59284174 -0.6500752 ]\n",
      "  [ 1.3292464  -0.593164   -0.6500752 ]\n",
      "  [ 1.0869949  -0.5931909  -0.6500752 ]]\n",
      "\n",
      " [[ 1.2208627  -0.5928015  -0.6500752 ]\n",
      "  [ 2.67722    -0.59336543 -0.6500752 ]\n",
      "  [ 2.2388887  -0.59302974 -0.6500752 ]\n",
      "  ...\n",
      "  [ 1.3292464  -0.593164   -0.6500752 ]\n",
      "  [ 1.0869949  -0.5931909  -0.6500752 ]\n",
      "  [ 1.9377234  -0.5932446  -0.6500752 ]]\n",
      "\n",
      " [[ 2.67722    -0.59336543 -0.6500752 ]\n",
      "  [ 2.2388887  -0.59302974 -0.6500752 ]\n",
      "  [ 1.7224554  -0.59328485 -0.6500752 ]\n",
      "  ...\n",
      "  [ 1.0869949  -0.5931909  -0.6500752 ]\n",
      "  [ 1.9377234  -0.5932446  -0.6500752 ]\n",
      "  [ 1.4257872  -0.5931909  -0.6500752 ]]]\n"
     ]
    }
   ],
   "source": [
    "#Data.train[0].shape\n",
    "print(Data.train[0])\n",
    "#type(Data.train)\n",
    "#print(len(Data.train[0]))\n",
    "#Data.train\n",
    "#n_classes = len(np.unique(Data.train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Root relative squared error\n",
    "def tf_rse(y_true, y_pred):\n",
    "    #\n",
    "    # The formula is:\n",
    "    #           K.sqrt(K.sum(K.square(y_true - y_pred)))     \n",
    "    #    RSE = -----------------------------------------------\n",
    "    #           K.sqrt(K.sum(K.square(y_true_mean - y_true)))       \n",
    "    #\n",
    "    #           K.sqrt(K.sum(K.square(y_true - y_pred))/(N-1))\n",
    "    #        = ----------------------------------------------------\n",
    "    #           K.sqrt(K.sum(K.square(y_true_mean - y_true)/(N-1)))\n",
    "    #\n",
    "    #\n",
    "    #           K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    #        = ------------------------------------------\n",
    "    #           K.std(y_true)\n",
    "    #\n",
    "    num = K.sqrt(K.mean(K.square(y_true - y_pred), axis=None))\n",
    "    den = K.std(y_true, axis=None)\n",
    "    \n",
    "    return num / den\n",
    "\n",
    "def rse_test1(y_true, y_pred):\n",
    "    return K.square(y_true - y_pred)\n",
    "\n",
    "def rse_test2(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_true - y_pred), axis=None))\n",
    "\n",
    "def rse_test3(y_true, y_pred):\n",
    "    return K.std(y_true, axis=None)\n",
    "\n",
    "def rse(y_true, y_pred):\n",
    "    #\n",
    "    # The formula is:\n",
    "    #           K.sqrt(K.sum(K.square(y_true - y_pred)))     \n",
    "    #    RSE = -----------------------------------------------\n",
    "    #           K.sqrt(K.sum(K.square(y_true_mean - y_true)))       \n",
    "    #\n",
    "    #           K.sqrt(K.sum(K.square( y_true - y_pred))/(N-1))\n",
    "    #        = ----------------------------------------------------\n",
    "    #           K.sqrt(K.sum(K.square(y_true_mean - y_true)/(N-1)))\n",
    "    #\n",
    "    #\n",
    "    #           K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    #        = ------------------------------------------\n",
    "    #           K.std(y_true)\n",
    "    #\n",
    "    num = np.sqrt(np.mean(np.square(y_true - y_pred), axis=None))\n",
    "    den = np.std(y_true, axis=None)\n",
    "    \n",
    "    return num / den\n",
    "\n",
    "def tf_corr(y_true, y_pred):\n",
    "    #\n",
    "    # This function calculates the correlation between the true and the predicted outputs\n",
    "    #\n",
    "    num1 = y_true - K.mean(y_true, axis=0)\n",
    "    num2 = y_pred - K.mean(y_pred, axis=0)\n",
    "    \n",
    "    num  = K.mean(num1 * num2, axis=0)\n",
    "    den  = K.std(y_true, axis=0) * K.std(y_pred, axis=0)\n",
    "    \n",
    "    return K.mean(num / den)\n",
    "\n",
    "def corr(y_true, y_pred):\n",
    "    #\n",
    "    # This function calculates the correlation between the true and the predicted outputs\n",
    "    #\n",
    "    num1 = y_true - np.mean(y_true, axis=0)\n",
    "    num2 = y_pred - np.mean(y_pred, axis=0)\n",
    "    \n",
    "    num  = np.mean(num1 * num2, axis=0)\n",
    "    den  = np.std(y_true, axis=0) * np.std(y_pred, axis=0)\n",
    "    \n",
    "    return np.mean(num / den)\n",
    "\n",
    "def single_rse(y_true, y_pred):\n",
    "    #\n",
    "    # The formula is:\n",
    "    #           K.sqrt(K.sum(K.square(y_true - y_pred)))     \n",
    "    #    RSE = -----------------------------------------------\n",
    "    #           K.sqrt(K.sum(K.square(y_true_mean - y_true)))       \n",
    "    #\n",
    "    #           K.sqrt(K.sum(K.square(y_true - y_pred))/(N-1))\n",
    "    #        = ----------------------------------------------------\n",
    "    #           K.sqrt(K.sum(K.square(y_true_mean - y_true)/(N-1)))\n",
    "    #\n",
    "    #\n",
    "    #           K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    #        = ------------------------------------------\n",
    "    #           K.std(y_true)\n",
    "    #\n",
    "    num = K.sqrt(K.mean(K.square(y_true[:,0] - y_pred[:,0]), axis=None))\n",
    "    den = K.std(y_true, axis=None)\n",
    "    \n",
    "    return num / den\n",
    "\n",
    "\n",
    "def single_corr(y_true, y_pred):\n",
    "    #\n",
    "    # This function calculates the correlation between the true and the predicted outputs\n",
    "    #\n",
    "    num1 = y_true[:,0] - K.mean(y_true[:,0], axis=0)\n",
    "    num2 = y_pred[:,0] - K.mean(y_pred[:,0], axis=0)\n",
    "    \n",
    "    num  = K.mean(num1 * num2, axis=0)\n",
    "    den  = K.std(y_true[:,0], axis=0) * K.std(y_pred[:,0], axis=0)\n",
    "    \n",
    "    return K.mean(num / den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "        )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = tf.keras.Input(shape=(10,3))\n",
    "#     x = tf.keras.layers.Flatten()(inputs)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(1,activation='linear')(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f\"data.train: {Data.train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_history = model.fit\n",
    "# train_history.history['accuracy']\n",
    "# plt.plot(train_history.history['accuracy'])\n",
    "# plt.plot(train_history.history['val_accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10, 3)]      0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 10, 3)       6           ['input_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 10, 3)       243         ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 10, 3)        0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 10, 3)       0           ['dropout[0][0]',                \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 10, 3)       6           ['tf.__operators__.add[0][0]']   \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 10, 4)        16          ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 10, 4)        0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 10, 3)        15          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 10, 3)       0           ['conv1d_1[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 10, 3)       6           ['tf.__operators__.add_1[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 10, 3)       243         ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 10, 3)        0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 10, 3)       0           ['dropout_2[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 10, 3)       6           ['tf.__operators__.add_2[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 10, 4)        16          ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 10, 4)        0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 10, 3)        15          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 10, 3)       0           ['conv1d_3[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 10, 3)       6           ['tf.__operators__.add_3[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 10, 3)       243         ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 10, 3)        0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 10, 3)       0           ['dropout_4[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 10, 3)       6           ['tf.__operators__.add_4[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 10, 4)        16          ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 10, 4)        0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 10, 3)        15          ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 10, 3)       0           ['conv1d_5[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 10, 3)       6           ['tf.__operators__.add_5[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 10, 3)       243         ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 10, 3)        0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 10, 3)       0           ['dropout_6[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 10, 3)       6           ['tf.__operators__.add_6[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 10, 4)        16          ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 10, 4)        0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 10, 3)        15          ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 10, 3)       0           ['conv1d_7[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 10)          0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          1408        ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,681\n",
      "Trainable params: 2,681\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "182/182 [==============================] - 4s 11ms/step - loss: 0.9968 - tf_rse: 1.0169 - tf_corr: 0.2379 - mean_absolute_error: 0.7959 - mean_squared_error: 0.9968 - root_mean_squared_error: 0.9984 - val_loss: 0.6303 - val_tf_rse: 1.2886 - val_tf_corr: 0.4636 - val_mean_absolute_error: 0.6334 - val_mean_squared_error: 0.6303 - val_root_mean_squared_error: 0.7939\n",
      "Epoch 2/200\n",
      "182/182 [==============================] - 2s 10ms/step - loss: 0.6908 - tf_rse: 0.8478 - tf_corr: 0.5719 - mean_absolute_error: 0.6554 - mean_squared_error: 0.6908 - root_mean_squared_error: 0.8311 - val_loss: 0.3903 - val_tf_rse: 1.0272 - val_tf_corr: 0.6017 - val_mean_absolute_error: 0.4911 - val_mean_squared_error: 0.3903 - val_root_mean_squared_error: 0.6247\n",
      "Epoch 3/200\n",
      "182/182 [==============================] - 2s 10ms/step - loss: 0.5379 - tf_rse: 0.7522 - tf_corr: 0.6877 - mean_absolute_error: 0.5795 - mean_squared_error: 0.5379 - root_mean_squared_error: 0.7334 - val_loss: 0.2543 - val_tf_rse: 0.8338 - val_tf_corr: 0.6644 - val_mean_absolute_error: 0.3870 - val_mean_squared_error: 0.2543 - val_root_mean_squared_error: 0.5042\n",
      "Epoch 4/200\n",
      "  1/182 [..............................] - ETA: 1s - loss: 0.4597 - tf_rse: 0.6397 - tf_corr: 0.7724 - mean_absolute_error: 0.5135 - mean_squared_error: 0.4597 - root_mean_squared_error: 0.6780"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-1845b47abc70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     model.fit(\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#f = open(\"TransformerResults2.txt\", \"w\")\n",
    "results = []\n",
    "testing_head_size = [4,2,3,5,6]\n",
    "testing_num_heads = [4,3] #[4,2,3,5,6]\n",
    "testing_ff_dim = [4,3,4,5,6]\n",
    "testing_num_transformer_blocks = [4,6] #[2,3,4,5,6]\n",
    "# for x in testing_head_size:\n",
    "#     tempResults = []\n",
    "    \n",
    "#     input_shape = (10,3)\n",
    "#     #print(f\"data.train: {type(Data.train)}\")\n",
    "#     model = build_model(\n",
    "#         input_shape,\n",
    "#         head_size=x,\n",
    "#         num_heads=4,\n",
    "#         ff_dim=4,\n",
    "#         num_transformer_blocks=4,\n",
    "#         mlp_units=[128],\n",
    "#         mlp_dropout=0.4,\n",
    "#         dropout=0.25,\n",
    "#     )\n",
    "#     testCase = \"head_size \" + str(x)\n",
    "#     tempResults.append(testCase)\n",
    "\n",
    "#     model.compile(\n",
    "#         loss=tf.losses.MeanSquaredError(),\n",
    "#         optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "#         metrics=[tf_rse, tf_corr, tf.metrics.MeanAbsoluteError(), tf.metrics.MeanSquaredError(), tf.metrics.RootMeanSquaredError()]\n",
    "#     )\n",
    "#     model.summary()\n",
    "\n",
    "#     callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "#     model.fit(\n",
    "#         Data.train[0][:,:,:], Data.train[1][:,0],\n",
    "#         validation_data=(Data.valid[0][:,:,:], Data.valid[1][:,0]),\n",
    "#         epochs=2000,\n",
    "#         batch_size=64,\n",
    "#     )\n",
    "#     tempResults.append(model.evaluate(Data.test[0][:,:,:], Data.test[1][:,0], verbose=1))\n",
    "#     results.append(tempResults)\n",
    "for x in testing_num_heads:\n",
    "    tempResults = []\n",
    "    \n",
    "    input_shape = (10,3)\n",
    "    #print(f\"data.train: {type(Data.train)}\")\n",
    "    model = build_model(\n",
    "        input_shape,\n",
    "        head_size=4,\n",
    "        num_heads=x,\n",
    "        ff_dim=4,\n",
    "        num_transformer_blocks=4,\n",
    "        mlp_units=[128],\n",
    "        mlp_dropout=0.4,\n",
    "        dropout=0.25,\n",
    "    )\n",
    "    testCase = \"num_heads \" + str(x)\n",
    "    tempResults.append(testCase)\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.losses.MeanSquaredError(),\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        metrics=[tf_rse, tf_corr, tf.metrics.MeanAbsoluteError(), tf.metrics.MeanSquaredError(), tf.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "    model.fit(\n",
    "        Data.train[0][:,:,:], Data.train[1][:,0],\n",
    "        validation_data=(Data.valid[0][:,:,:], Data.valid[1][:,0]),\n",
    "        epochs=200,\n",
    "        batch_size=64,\n",
    "    )\n",
    "    tempResults.append(model.evaluate(Data.test[0][:,:,:], Data.test[1][:,0], verbose=1))\n",
    "    results.append(tempResults)\n",
    "# for x in testing_ff_dim:\n",
    "#     tempResults = []\n",
    "    \n",
    "#     input_shape = (10,3)\n",
    "#     #print(f\"data.train: {type(Data.train)}\")\n",
    "#     model = build_model(\n",
    "#         input_shape,\n",
    "#         head_size=4,\n",
    "#         num_heads=4,\n",
    "#         ff_dim=x,\n",
    "#         num_transformer_blocks=4,\n",
    "#         mlp_units=[128],\n",
    "#         mlp_dropout=0.4,\n",
    "#         dropout=0.25,\n",
    "#     )\n",
    "#     testCase = \"ff_dim \" + str(x)\n",
    "#     tempResults.append(testCase)\n",
    "\n",
    "#     model.compile(\n",
    "#         loss=tf.losses.MeanSquaredError(),\n",
    "#         optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "#         metrics=[tf_rse, tf_corr, tf.metrics.MeanAbsoluteError(), tf.metrics.MeanSquaredError(), tf.metrics.RootMeanSquaredError()]\n",
    "#     )\n",
    "#     model.summary()\n",
    "\n",
    "#     callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "#     model.fit(\n",
    "#         Data.train[0][:,:,:], Data.train[1][:,0],\n",
    "#         validation_data=(Data.valid[0][:,:,:], Data.valid[1][:,0]),\n",
    "#         epochs=2000,\n",
    "#         batch_size=64,\n",
    "#     )\n",
    "#     tempResults.append(model.evaluate(Data.test[0][:,:,:], Data.test[1][:,0], verbose=1))\n",
    "#     results.append(tempResults)\n",
    "for x in testing_num_transformer_blocks:\n",
    "    tempResults = []\n",
    "    \n",
    "    input_shape = (10,3)\n",
    "    #print(f\"data.train: {type(Data.train)}\")\n",
    "    model = build_model(\n",
    "        input_shape,\n",
    "        head_size=4,\n",
    "        num_heads=4,\n",
    "        ff_dim=4,\n",
    "        num_transformer_blocks=x,\n",
    "        mlp_units=[128],\n",
    "        mlp_dropout=0.4,\n",
    "        dropout=0.25,\n",
    "    )\n",
    "    testCase = \"num_transformer_blocks \" + str(x)\n",
    "    tempResults.append(testCase)\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.losses.MeanSquaredError(),\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        metrics=[tf_rse, tf_corr, tf.metrics.MeanAbsoluteError(), tf.metrics.MeanSquaredError(), tf.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "    model.fit(\n",
    "        Data.train[0][:,:,:], Data.train[1][:,0],\n",
    "        validation_data=(Data.valid[0][:,:,:], Data.valid[1][:,0]),\n",
    "        epochs=200,\n",
    "        batch_size=64,\n",
    "    )\n",
    "    tempResults.append(model.evaluate(Data.test[0][:,:,:], Data.test[1][:,0], verbose=1))\n",
    "    results.append(tempResults)\n",
    "    #for i in results:\n",
    "        #print(i)\n",
    "        #f.write(str(i))\n",
    "        #f.write(\"Hello\")\n",
    "#f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data.train[0][:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.train[1][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training = np.array([np.array(xi) for xi in Data.train])\n",
    "latency = []\n",
    "throughput = []\n",
    "tcp_buffer_size = []\n",
    "counter = 0\n",
    "for x in range(1):\n",
    "    \n",
    "    for y in range(10):\n",
    "        latency_list = []\n",
    "        throughput_list = []\n",
    "        tcp_buffer_size_list = []\n",
    "        for z in Data.train[x][y]:\n",
    "            #print(f\"y is: {z}\")\n",
    "            latency_list.append(z[0])\n",
    "            throughput_list.append(z[1])\n",
    "            tcp_buffer_size_list.append(z[2])\n",
    "        latency.append(latency_list)\n",
    "        throughput.append(throughput_list)\n",
    "        tcp_buffer_size.append(tcp_buffer_size_list)\n",
    "    break\n",
    "#print(f\"latency: {latency[0]}\")\n",
    "training = np.array([np.array(xi) for xi in latency])\n",
    "training = Data.train.reshape((Data.train[0].shape[0], data.train[0].shape[1], 1))\n",
    "#x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "#print(f\"data train is: {training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_performance = {}\n",
    "performance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Root relative squared error\n",
    "def tf_rse(y_true, y_pred):\n",
    "    #\n",
    "    # The formula is:\n",
    "    #           K.sqrt(K.sum(K.square(y_true - y_pred)))     \n",
    "    #    RSE = -----------------------------------------------\n",
    "    #           K.sqrt(K.sum(K.square(y_true_mean - y_true)))       \n",
    "    #\n",
    "    #           K.sqrt(K.sum(K.square(y_true - y_pred))/(N-1))\n",
    "    #        = ----------------------------------------------------\n",
    "    #           K.sqrt(K.sum(K.square(y_true_mean - y_true)/(N-1)))\n",
    "    #\n",
    "    #\n",
    "    #           K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    #        = ------------------------------------------\n",
    "    #           K.std(y_true)\n",
    "    #\n",
    "    num = K.sqrt(K.mean(K.square(y_true - y_pred), axis=None))\n",
    "    den = K.std(y_true, axis=None)\n",
    "    \n",
    "    return num / den\n",
    "\n",
    "def rse_test1(y_true, y_pred):\n",
    "    return K.square(y_true - y_pred)\n",
    "\n",
    "def rse_test2(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_true - y_pred), axis=None))\n",
    "\n",
    "def rse_test3(y_true, y_pred):\n",
    "    return K.std(y_true, axis=None)\n",
    "\n",
    "def rse(y_true, y_pred):\n",
    "    #\n",
    "    # The formula is:\n",
    "    #           K.sqrt(K.sum(K.square(y_true - y_pred)))     \n",
    "    #    RSE = -----------------------------------------------\n",
    "    #           K.sqrt(K.sum(K.square(y_true_mean - y_true)))       \n",
    "    #\n",
    "    #           K.sqrt(K.sum(K.square( y_true - y_pred))/(N-1))\n",
    "    #        = ----------------------------------------------------\n",
    "    #           K.sqrt(K.sum(K.square(y_true_mean - y_true)/(N-1)))\n",
    "    #\n",
    "    #\n",
    "    #           K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    #        = ------------------------------------------\n",
    "    #           K.std(y_true)\n",
    "    #\n",
    "    num = np.sqrt(np.mean(np.square(y_true - y_pred), axis=None))\n",
    "    den = np.std(y_true, axis=None)\n",
    "    \n",
    "    return num / den\n",
    "\n",
    "def tf_corr(y_true, y_pred):\n",
    "    #\n",
    "    # This function calculates the correlation between the true and the predicted outputs\n",
    "    #\n",
    "    num1 = y_true - K.mean(y_true, axis=0)\n",
    "    num2 = y_pred - K.mean(y_pred, axis=0)\n",
    "    \n",
    "    num  = K.mean(num1 * num2, axis=0)\n",
    "    den  = K.std(y_true, axis=0) * K.std(y_pred, axis=0)\n",
    "    \n",
    "    return K.mean(num / den)\n",
    "\n",
    "def corr(y_true, y_pred):\n",
    "    #\n",
    "    # This function calculates the correlation between the true and the predicted outputs\n",
    "    #\n",
    "    num1 = y_true - np.mean(y_true, axis=0)\n",
    "    num2 = y_pred - np.mean(y_pred, axis=0)\n",
    "    \n",
    "    num  = np.mean(num1 * num2, axis=0)\n",
    "    den  = np.std(y_true, axis=0) * np.std(y_pred, axis=0)\n",
    "    \n",
    "    return np.mean(num / den)\n",
    "\n",
    "def single_rse(y_true, y_pred):\n",
    "    #\n",
    "    # The formula is:\n",
    "    #           K.sqrt(K.sum(K.square(y_true - y_pred)))     \n",
    "    #    RSE = -----------------------------------------------\n",
    "    #           K.sqrt(K.sum(K.square(y_true_mean - y_true)))       \n",
    "    #\n",
    "    #           K.sqrt(K.sum(K.square(y_true - y_pred))/(N-1))\n",
    "    #        = ----------------------------------------------------\n",
    "    #           K.sqrt(K.sum(K.square(y_true_mean - y_true)/(N-1)))\n",
    "    #\n",
    "    #\n",
    "    #           K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    #        = ------------------------------------------\n",
    "    #           K.std(y_true)\n",
    "    #\n",
    "    num = K.sqrt(K.mean(K.square(y_true[:,0] - y_pred[:,0]), axis=None))\n",
    "    den = K.std(y_true, axis=None)\n",
    "    \n",
    "    return num / den\n",
    "\n",
    "\n",
    "def single_corr(y_true, y_pred):\n",
    "    #\n",
    "    # This function calculates the correlation between the true and the predicted outputs\n",
    "    #\n",
    "    num1 = y_true[:,0] - K.mean(y_true[:,0], axis=0)\n",
    "    num2 = y_pred[:,0] - K.mean(y_pred[:,0], axis=0)\n",
    "    \n",
    "    num  = K.mean(num1 * num2, axis=0)\n",
    "    den  = K.std(y_true[:,0], axis=0) * K.std(y_pred[:,0], axis=0)\n",
    "    \n",
    "    return K.mean(num / den)\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "  return np.mean(np.abs(y_true - y_pred) / np.abs(y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                patience=10,\n",
    "                                                mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.train[0][:,:,0:3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.train[1][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_stats = {}\n",
    "accuracy_stats['rmse'] = {}\n",
    "accuracy_stats['rse'] = {}\n",
    "accuracy_stats['corr'] = {}\n",
    "accuracy_stats['accuracy'] = {}\n",
    "accuracy_stats['mae'] = {}\n",
    "accuracy_stats['predicted'] = {}\n",
    "accuracy_stats['mse'] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple MLP\n",
    "\n",
    "Here we are testing the effectiveness of a simple ANN (MLP) on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(10,3)),\n",
    "    tf.keras.layers.Dense(units=15),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=15),\n",
    "    tf.keras.layers.Dense(units=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                optimizer=tf.optimizers.Adam(),\n",
    "                metrics=[tf_rse, tf_corr, tf.metrics.MeanAbsoluteError(), tf.metrics.MeanSquaredError(), tf.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.train[1][:,0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.fit(Data.train[0][:,:,:], Data.train[1][:,0].reshape(Data.train[1][:,0].shape[0],1), epochs=100, validation_data=(Data.valid[0][:,:,:], Data.valid[1][:,0].reshape(Data.valid[1][:,0].shape[0],1)), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_stats = linear.evaluate(Data.test[0], Data.test[1][:,0].reshape(2933,1))\n",
    "linear_stats = linear.evaluate(Data.test[0], Data.test[1][:,0].reshape(25230,1))\n",
    "\n",
    "linear_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_predicted = linear.predict(Data.test[0][:,:,:])\n",
    "\n",
    "np.save('predicted_results/mlp_predicted.npy', linear_predicted)\n",
    "np.savetxt('predicted_results/mlp_predicted.txt', linear_predicted)\n",
    "linear.save_weights('./checkpoints/linear_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr\n",
    "import matplotlib.cm as cm\n",
    "Data.test[1][0:25230,0]\n",
    "linear_predicted[0:25230]\n",
    "inquartileRange = iqr(Data.test[1][0:25230,0],interpolation = 'midpoint')\n",
    "print(inquartileRange)\n",
    "higherRange = inquartileRange * 1.5\n",
    "\n",
    "import numpy as np\n",
    "# First quartile (Q1)\n",
    "#print(Data.test[])\n",
    "Q1 = np.median(Data.test[1][:12615,0])\n",
    "  \n",
    "# Third quartile (Q3)\n",
    "Q3 = np.median(Data.test[1][12615:,0])\n",
    "  \n",
    "# Interquartile range (IQR)\n",
    "IQR = Q3 - Q1\n",
    "print(f\"IQR: {IQR}\")\n",
    "# Quartile Deviation\n",
    "qd = IQR / 2\n",
    "  \n",
    "print(f\"Quartile Deviation: {qd}\")\n",
    "highRange = IQR + qd\n",
    "lowRange = IQR - qd\n",
    "print(f\"High Range: {highRange}\")\n",
    "print(f\"Low Range: {lowRange}\")\n",
    "# plt.plot(Data.test[1][0:25230,0])\n",
    "# print(linear_predicted[0:25230])\n",
    "# anomolyList =[]\n",
    "# anomolyListXCoords =[]\n",
    "# count = 0\n",
    "# for x in Data.test[1][0:25230,0]:\n",
    "#     if x > 2 or x <-1:\n",
    "#         anomolyList.append([x])\n",
    "#         anomolyListXCoords.append(count)\n",
    "#     count = count + 1\n",
    "# colors = cm.rainbow(np.linspace(1, 1, 2119))\n",
    "# #print(anomolyList)\n",
    "# #plt.scatter(linear_predicted[0:25230])\n",
    "# plt.scatter(anomolyListXCoords, anomolyList, c=colors)\n",
    "# plt.legend([\"Actual\", \"Anomoly Points\"])\n",
    "# plt.show()\n",
    "print(Data.test[0][1,:])\n",
    "#Data.test[x] shows data in the format [throughput, latency, bufferSize] . Focus Anomoly detection on both Throughput and Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import matplotlib.cm as cm\n",
    "dataRange = 500\n",
    "Data.test[1][0:25230,0]\n",
    "linear_predicted[0:25230]\n",
    "plt.plot(Data.test[1][0:dataRange,0])\n",
    "#print(linear_predicted[0:25230])\n",
    "anomalyList =[]\n",
    "anomalyListXCoords =[]\n",
    "count = 0\n",
    "total=0\n",
    "for x in range(dataRange):\n",
    "    difference = abs(Data.test[1][x,0]-linear_predicted[x])\n",
    "    total = difference + total\n",
    "    count += 1\n",
    "total = total / count\n",
    "print(f\"The Average is: {total}\")\n",
    "count = 0\n",
    "for x in range(dataRange):\n",
    "    difference = abs(Data.test[1][x,0]-linear_predicted[x])\n",
    "    if difference > 3*total:\n",
    "        anomalyList.append(Data.test[1][x,0])\n",
    "        anomalyListXCoords.append(count)\n",
    "    count += 1\n",
    "percentageAnomalous =  round(len(anomalyList)/dataRange *100)\n",
    "print(f\"The percentage of total anomalous points was: {percentageAnomalous}%\")\n",
    "colors = cm.rainbow(np.linspace(1, 1, len(anomalyList)))\n",
    "print(f\"The len of anomalyList: {len(anomalyList)}\")\n",
    "#print(anomolyList)\n",
    "#plt.scatter(linear_predicted[0:25230])\n",
    "#print(f\"The anomalyList: {anomalyList}\\nThe anomalyListXCoords: {anomalyListXCoords}\")\n",
    "\n",
    "plt.scatter(anomalyListXCoords, anomalyList, c=colors)\n",
    "#plt.scatter([1762, 4443, 19902, 23501, 25112], [3.7193446, 4.0872087, 4.4067316, 3.9032767, 4.3992643])#, c=colors)\n",
    "\n",
    "plt.legend([\"Actual\", \"Anomoly Points\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Data.test[1][0:25230,0])\n",
    "plt.plot(linear_predicted[0:25230])\n",
    "plt.legend([\"Actual\", \"Predicted\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "mse = mean_squared_error(Data.test[1][:,0], linear_predicted.flatten())\n",
    "rmse = sqrt(mean_squared_error(Data.test[1][:,0], linear_predicted.flatten()))\n",
    "mae = mean_absolute_error(Data.test[1][:,0], linear_predicted.flatten())\n",
    "rse_val = rse(Data.test[1][:,0], linear_predicted.flatten())\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'RSE: {rse_val}')\n",
    "\n",
    "accuracy_stats['rmse']['mlp'] = rmse\n",
    "accuracy_stats['rse']['mlp'] = rse_val\n",
    "# accuracy_stats['corr']['mlp'] = corr\n",
    "# accuracy_stats['accuracy']['mlp'] = acc\n",
    "accuracy_stats['mae']['mlp'] = mae\n",
    "accuracy_stats['predicted']['mlp'] = linear_predicted\n",
    "accuracy_stats['mse']['mlp'] = mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMPLE LSTM\n",
    "\n",
    "Here we run our data through a simple lstm for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.train[0][:,:,0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.train[0][:,:,0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lstm network\n",
    "\n",
    "# lstm_model = tf.keras.models.Sequential([\n",
    "#     # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "#     tf.keras.layers.GRU(32, input_shape=(10), return_sequences=False),\n",
    "#     # Shape => [batch, time, features]\n",
    "#     tf.keras.layers.Dense(units=1)\n",
    "# ])\n",
    "\n",
    "ts_inputs = tf.keras.Input(shape=(10,3))\n",
    "x = tf.keras.layers.LSTM(units=250, dropout=0.1, recurrent_dropout=0.1)(ts_inputs)\n",
    "# x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = tf.keras.layers.Dense(1,activation='linear')(x)\n",
    "lstm_model = tf.keras.Model(inputs=ts_inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                optimizer=tf.optimizers.Adam(),\n",
    "                metrics=[tf_rse, tf_corr, tf.metrics.MeanAbsoluteError(), tf.metrics.MeanSquaredError(), tf.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = lstm_model.fit(Data.train[0][:,:,:], Data.train[1][:,0], \n",
    "               epochs=100, \n",
    "               validation_data=(Data.valid[0][:,:,:], Data.valid[1][:,0]), \n",
    "               callbacks=[early_stopping],\n",
    "               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_stats = lstm_model.evaluate(Data.test[0][:,:,:], Data.test[1][:,0])\n",
    "lstm_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_predicted = lstm_model.predict(Data.test[0][:,:,:]).flatten()\n",
    "\n",
    "np.save('predicted_results/lstm_predicted.npy', lstm_predicted)\n",
    "np.savetxt('predicted_results/lstm_predicted.txt', lstm_predicted)\n",
    "lstm_model.save_weights('./checkpoints/lstm_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_predicted = np.load('predicted_results/lstm_predicted.npy')*Data.normalize_std[0] + Data.normalize_mean[0]\n",
    "data_true = Data.test[1][:,0]*Data.normalize_std[0] + Data.normalize_mean[0]\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "  return np.mean(np.abs(y_true - y_pred) / np.abs(y_true)) * 100\n",
    "lstm_mape = mean_absolute_percentage_error(data_true, lstm_predicted)\n",
    "lstm_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Data.test[1][0:50,0])\n",
    "plt.plot(lstm_predicted[0:50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 10))\n",
    "plt.plot(Data.test[1][:,0],label=\"actual\")\n",
    "plt.plot(lstm_predicted, color='r',label=\"predicted\")\n",
    "plt.legend(loc='best', fontsize='xx-large')\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "mse = mean_squared_error(Data.test[1][:,0], lstm_predicted)\n",
    "rmse = sqrt(mean_squared_error(Data.test[1][:,0], lstm_predicted))\n",
    "mae = mean_absolute_error(Data.test[1][:,0], lstm_predicted)\n",
    "rse_val = rse(Data.test[1][:,0], lstm_predicted)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'RSE: {rse_val}')\n",
    "\n",
    "accuracy_stats['rmse']['lstm'] = rmse\n",
    "accuracy_stats['rse']['lstm'] = rse_val\n",
    "# accuracy_stats['corr']['mlp'] = corr\n",
    "# accuracy_stats['accuracy']['mlp'] = acc\n",
    "accuracy_stats['mae']['lstm'] = mae\n",
    "accuracy_stats['predicted']['lstm'] = lstm_predicted\n",
    "accuracy_stats['mse']['lstm'] = mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.test[1][:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple GRU\n",
    "\n",
    "Do the same thing but with a GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_inputs = tf.keras.Input(shape=(10,3))\n",
    "x = tf.keras.layers.GRU(units=250, dropout=0.1)(ts_inputs)\n",
    "# x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = tf.keras.layers.Dense(1,activation='linear')(x)\n",
    "gru_model = tf.keras.Model(inputs=ts_inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                  optimizer=tf.optimizers.Adam(),\n",
    "                  metrics=[tf_rse, tf_corr, tf.metrics.MeanAbsoluteError(), tf.metrics.MeanSquaredError(), tf.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model.fit(Data.train[0][:,:,:], Data.train[1][:,0], \n",
    "               epochs=100, \n",
    "               validation_data=(Data.valid[0][:,:,:], Data.valid[1][:,0]), \n",
    "               callbacks=[early_stopping],\n",
    "               shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model.evaluate(Data.test[0][:,:,:], Data.test[1][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_predicted = gru_model.predict(Data.test[0][:,:,:]).flatten()\n",
    "\n",
    "np.save('predicted_results/gru_predicted.npy', gru_predicted)\n",
    "np.savetxt('predicted_results/gru_predicted.txt', gru_predicted)\n",
    "gru_model.save_weights('./checkpoints/gru_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_predicted = np.load('predicted_results/gru_predicted.npy')*Data.normalize_std[0] + Data.normalize_mean[0]\n",
    "data_true = Data.test[1][:,0]*Data.normalize_std[0] + Data.normalize_mean[0]\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "  return np.mean(np.abs(y_true - y_pred) / np.abs(y_true)) * 100\n",
    "gru_mape = mean_absolute_percentage_error(data_true, gru_predicted)\n",
    "gru_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Data.test[1][0:100,0], label='actual')\n",
    "plt.plot(gru_predicted[0:100], label=\"predicted\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "mse = mean_squared_error(Data.test[1][:,0], gru_predicted)\n",
    "rmse = sqrt(mean_squared_error(Data.test[1][:,0], gru_predicted))\n",
    "mae = mean_absolute_error(Data.test[1][:,0], gru_predicted)\n",
    "rse_val = rse(Data.test[1][:,0], gru_predicted)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'RSE: {rse_val}')\n",
    "\n",
    "accuracy_stats['rmse']['gru'] = rmse\n",
    "accuracy_stats['rse']['gru'] = rse_val\n",
    "# accuracy_stats['corr']['mlp'] = corr\n",
    "# accuracy_stats['accuracy']['mlp'] = acc\n",
    "accuracy_stats['mae']['gru'] = mae\n",
    "accuracy_stats['predicted']['gru'] = gru_predicted\n",
    "accuracy_stats['mse']['gru'] = mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "\n",
    "\n",
    "# cnn_model = Sequential()\n",
    "# cnn_model.add(Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(10, 3)))\n",
    "# cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "# cnn_model.add(Flatten())\n",
    "# cnn_model.add(Dense(50, activation='relu'))\n",
    "# cnn_model.add(Dense(1))\n",
    "# cnn_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv1D(filters=30, kernel_size=5, activation='relu', padding='SAME', strides=1, input_shape=(10, 3)))\n",
    "cnn_model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "cnn_model.add(Conv1D(filters=45, kernel_size=5, activation='relu', padding='SAME', strides=1))\n",
    "cnn_model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "cnn_model.add(Conv1D(filters=60, kernel_size=5, activation='relu', padding='SAME', strides=1))\n",
    "cnn_model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(120, activation='relu'))\n",
    "cnn_model.add(Dense(1))\n",
    "# cnn_model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "              optimizer=tf.optimizers.Adam(),\n",
    "              metrics=[tf_rse, tf_corr, tf.metrics.MeanAbsoluteError(), tf.metrics.MeanSquaredError(), tf.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.fit(Data.train[0][:,:,:], Data.train[1][:,0], \n",
    "               epochs=100, \n",
    "               validation_data=(Data.valid[0][:,:,:], Data.valid[1][:,0]), \n",
    "               callbacks=[early_stopping],\n",
    "               shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_stats = cnn_model.evaluate(Data.test[0][:,:,:], Data.test[1][:,0])\n",
    "cnn_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_predicted = cnn_model.predict(Data.test[0][:,:,:]).flatten()\n",
    "\n",
    "np.save('predicted_results/cnn_predicted.npy', cnn_predicted)\n",
    "np.savetxt('predicted_results/cnn_predicted.txt', cnn_predicted)\n",
    "cnn_model.save_weights('./checkpoints/cnn_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Data.test[1][0:100,0], label='actual')\n",
    "plt.plot(cnn_predicted[0:100], label=\"predicted\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "mse = mean_squared_error(Data.test[1][:,0], cnn_predicted)\n",
    "rmse = sqrt(mean_squared_error(Data.test[1][:,0], cnn_predicted))\n",
    "mae = mean_absolute_error(Data.test[1][:,0], cnn_predicted)\n",
    "rse_val = rse(Data.test[1][:,0], cnn_predicted)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'RSE: {rse_val}')\n",
    "\n",
    "accuracy_stats['rmse']['cnn'] = rmse\n",
    "accuracy_stats['rse']['cnn'] = rse_val\n",
    "# accuracy_stats['corr']['mlp'] = corr\n",
    "# accuracy_stats['accuracy']['mlp'] = acc\n",
    "accuracy_stats['mae']['cnn'] = mae\n",
    "accuracy_stats['predicted']['cnn'] = cnn_predicted\n",
    "accuracy_stats['mse']['cnn'] = mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_predicted = np.load('predicted_results/cnn_predicted.npy')*Data.normalize_std[0] + Data.normalize_mean[0]\n",
    "data_true = Data.test[1][:,0]*Data.normalize_std[0] + Data.normalize_mean[0]\n",
    "\n",
    "cnn_mape = mean_absolute_percentage_error(data_true, cnn_predicted)\n",
    "cnn_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-LSTM HYBRID MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, Conv2D, LSTM, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_model = Sequential()\n",
    "cnn_lstm_model.add(Conv1D(filters=64, kernel_size=5, activation='relu', padding='SAME', strides=1, input_shape=(10, 3)))\n",
    "cnn_lstm_model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "cnn_lstm_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='SAME', strides=1))\n",
    "cnn_lstm_model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "# cnn_lstm_model.add(Conv1D(filters=10, kernel_size=3, activation='relu', padding='SAME', strides=1))\n",
    "# cnn_lstm_model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "# cnn_model.add(Flatten())\n",
    "cnn_lstm_model.add(LSTM(32, dropout=0.1, recurrent_dropout=0.1, return_sequences=False))\n",
    "cnn_lstm_model.add(Dense(100, activation='relu'))\n",
    "cnn_lstm_model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                       optimizer=tf.optimizers.Adam(),\n",
    "                       metrics=[tf_rse, tf_corr, tf.metrics.MeanAbsoluteError(), tf.metrics.MeanSquaredError(), tf.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_model.fit(Data.train[0][:,:,:], Data.train[1][:,0], \n",
    "                   epochs=100, \n",
    "                   validation_data=(Data.valid[0][:,:,:], Data.valid[1][:,0]), \n",
    "                   callbacks=[early_stopping],\n",
    "                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm1_stats = cnn_lstm_model.evaluate(Data.test[0][:,:,:], Data.test[1][:,0])\n",
    "cnn_lstm1_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_predicted = cnn_lstm_model.predict(Data.test[0][:,:,:]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Data.test[1][0:100,0], label='actual')\n",
    "plt.plot(cnn_lstm_predicted[0:100], label=\"predicted\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "mse = mean_squared_error(Data.test[1][:,0], cnn_lstm_predicted)\n",
    "rmse = sqrt(mean_squared_error(Data.test[1][:,0], cnn_lstm_predicted))\n",
    "mae = mean_absolute_error(Data.test[1][:,0], cnn_lstm_predicted)\n",
    "rse_val = rse(Data.test[1][:,0], cnn_lstm_predicted)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'RSE: {rse_val}')\n",
    "\n",
    "accuracy_stats['rmse']['cnn_lstm'] = rmse\n",
    "accuracy_stats['rse']['cnn_lstm'] = rse_val\n",
    "# accuracy_stats['corr']['mlp'] = corr\n",
    "# accuracy_stats['accuracy']['mlp'] = acc\n",
    "accuracy_stats['mae']['cnn_lstm'] = mae\n",
    "accuracy_stats['predicted']['cnn_lstm'] = cnn_lstm_predicted\n",
    "accuracy_stats['mse']['cnn_lstm'] = mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                patience=10,\n",
    "                                                mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, LSTM, MaxPooling2D, Reshape, TimeDistributed, Input, Dropout, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Input(shape = (10,3))\n",
    "\n",
    "\n",
    "#CNN\n",
    "C = Reshape((10, 3, 1))(X)\n",
    "\n",
    "# Apply a Conv2D that will transform it into data of dimensions (batchsize, time, 1, NumofFilters)\n",
    "C = Conv2D(filters=200, kernel_size=(1, 3), kernel_initializer='glorot_uniform')(C)\n",
    "C = Dropout(0.2)(C)\n",
    "\n",
    "# Adjust data dimensions by removing axis=2 which is always equal to 1\n",
    "c_shape = K.int_shape(C)\n",
    "C = Reshape((c_shape[1], c_shape[3]))(C)\n",
    "\n",
    "# Apply a GRU layer (with activation set to 'relu' as per the paper) and take the returned states as result\n",
    "_, R = GRU(200, activation=\"relu\", return_sequences = False, return_state = True)(C)\n",
    "R    = Dropout(0.2)(R)\n",
    "Y = Flatten()(R)\n",
    "Y = Dense(3)(Y)\n",
    "cnn_lstm_model2 = Model(inputs = X, outputs = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_model2.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                       optimizer=tf.optimizers.Adam(),\n",
    "                       metrics=[tf_rse, tf_corr, tf.metrics.MeanAbsoluteError(), tf.metrics.MeanSquaredError(), tf.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(123456)\n",
    "cnn_lstm_model2.fit(Data.train[0][:,:,:], Data.train[1][:,0], \n",
    "                   epochs=10, \n",
    "                   validation_data=(Data.valid[0][:,:,:], Data.valid[1]), \n",
    "                   callbacks=[early_stopping],\n",
    "                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_model2.evaluate(Data.test[0][:,:,:], Data.test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_model2_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_model2_predicted = cnn_lstm_model2.predict(Data.test[0][:,:,:])\n",
    "\n",
    "np.save('predicted_results/cnn_lstm_predicted.npy', cnn_lstm_model2_predicted)\n",
    "np.savetxt('predicted_results/cnn_lstm_predicted.txt', cnn_lstm_model2_predicted)\n",
    "cnn_lstm_model.save_weights('./checkpoints/cnn_lstm_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Data.test[1][0:100,0], label='actual')\n",
    "plt.plot(cnn_lstm_model2_predicted[0:100,0], label=\"predicted\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "mse = mean_squared_error(Data.test[1][:,0], cnn_lstm_model2_predicted.flatten())\n",
    "rmse = sqrt(mean_squared_error(Data.test[1][:,0], cnn_lstm_model2_predicted.flatten()))\n",
    "mae = mean_absolute_error(Data.test[1][:,0], cnn_lstm_model2_predicted.flatten())\n",
    "rse_val = rse(Data.test[1][:,0], cnn_lstm_model2_predicted[:,0].flatten())\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'RSE: {rse_val}')\n",
    "\n",
    "accuracy_stats['rmse']['cnn_lstm2'] = rmse\n",
    "accuracy_stats['rse']['cnn_lstm2'] = rse_val\n",
    "# accuracy_stats['corr']['mlp'] = corr\n",
    "# accuracy_stats['accuracy']['mlp'] = acc\n",
    "accuracy_stats['mae']['cnn_lstm2'] = mae\n",
    "accuracy_stats['predicted']['cnn_lstm2'] = cnn_lstm_model2_predicted\n",
    "accuracy_stats['mse']['cnn_lstm2'] = mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTNET MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, model_from_json\n",
    "\n",
    "custom_objects = {\n",
    "        'PreSkipTrans': PreSkipTrans,\n",
    "        'PostSkipTrans': PostSkipTrans,\n",
    "        'PreARTrans': PreARTrans,\n",
    "        'PostARTrans': PostARTrans\n",
    "        }\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # print(\"CUSTOM LOSS BABBBY\")\n",
    "    # print(type(y_true))\n",
    "    # print(type(y_pred))\n",
    "    # print(y_true)\n",
    "    # print(y_true.shape)\n",
    "    # tf.print(y_true[:,0, sys.stdout)\n",
    "    return tf.keras.losses.mean_absolute_error(y_true[:,0], y_pred[:,0])\n",
    "\n",
    "file = 'models/model2.json'\n",
    "lstnet_model = None\n",
    "with open(file, \"r\") as json_file:\n",
    "  lstnet_model = model_from_json(json_file.read(), custom_objects=custom_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstnet_model.compile(loss=custom_loss,\n",
    "                       optimizer=tf.optimizers.Adam(),\n",
    "                       metrics=[single_rse, single_corr, tf_rse, tf_corr, tf.metrics.MeanAbsoluteError(), tf.metrics.MeanSquaredError(), tf.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstnet_model.fit(Data.train[0][:,:,:], Data.train[1][:,0], \n",
    "                   epochs=20, \n",
    "                   validation_data=(Data.valid[0][:,:,:], Data.valid[1][:,0]), \n",
    "                   callbacks=[early_stopping],\n",
    "                   shuffle=True,\n",
    "                   batch_size=91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstnet_model.evaluate(Data.test[0], Data.test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstnet_predicted = lstnet_model.predict(Data.test[0])\n",
    "\n",
    "np.save('predicted_results/lstnet_predicted.npy', lstnet_predicted)\n",
    "np.savetxt('predicted_results/lstnet_predicted.txt', lstnet_predicted)\n",
    "lstnet_model.save_weights('./checkpoints/lstnet_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Data.test[1][0:100,0], label='actual')\n",
    "plt.plot(lstnet_predicted[0:100, 0], label=\"predicted\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "print(f'MSE: {mean_squared_error(Data.test[1][:,0], lstnet_predicted[:,0])}')\n",
    "print(f'RMSE: {sqrt(mean_squared_error(Data.test[1][:,0], lstnet_predicted[:,0]))}')\n",
    "print(f'MAE: {mean_absolute_error(Data.test[1][:,0], lstnet_predicted[:,0])}')\n",
    "print(f'RSE: {rse(Data.test[1][:,0], lstnet_predicted[:,0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.test[1][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstnet_predicted[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "  return np.mean(np.abs(y_true - y_pred) / np.abs(y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(Data.test[1][:,0]*Data.normalize_std[0] + Data.normalize_mean[0], lstnet_predicted[:,0]*Data.normalize_std[0] + Data.normalize_mean[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between Different Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_predicted = np.load('predicted_results/mlp_predicted.npy')*Data.normalize_std[0] + Data.normalize_mean[0]\n",
    "lstm_predicted = np.load('predicted_results/lstm_predicted.npy')*Data.normalize_std[0] + Data.normalize_mean[0]\n",
    "gru_predicted = np.load('predicted_results/gru_predicted.npy')*Data.normalize_std[0] + Data.normalize_mean[0]\n",
    "cnn_predicted = np.load('predicted_results/cnn_predicted.npy')*Data.normalize_std[0] + Data.normalize_mean[0]\n",
    "cnn_lstm_predicted = np.load('predicted_results/cnn_lstm_predicted.npy')*Data.normalize_std[0] + Data.normalize_mean[0]\n",
    "lstnet_predicted = np.load('predicted_results/lstnet_predicted.npy')*Data.normalize_std[0] + Data.normalize_mean[0]\n",
    "\n",
    "data_true = Data.test[1][:,0]*Data.normalize_std[0] + Data.normalize_mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycler import cycler\n",
    "\n",
    "# Create cycler object. Use any styling from above you please\n",
    "monochrome = (cycler('color', ['k']) * cycler('linestyle', ['-', '--', ':']) * cycler('marker', ['^',',', '.']))\n",
    "\n",
    "# Print examples of output from cycler object. \n",
    "# A cycler object, when called, returns a `iter.cycle` object that iterates over items indefinitely\n",
    "\n",
    "default_cycler = (cycler(color=['black', 'r', 'b', 'y', 'teal', 'green', 'purple']) +\n",
    "                  cycler(linestyle=['-', '--', ':', '-.', '-.', '-', '--']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "cmap_1 = ListedColormap([\"darkorange\", \"gold\", \"lawngreen\", \"lightseagreen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 150\n",
    "end_time = 200\n",
    "plt.rcParams[\"figure.figsize\"] = (10,7)\n",
    "# plt.rc('axes', prop_cycle=default_cycler)\n",
    "# plt.style.use('grayscale')\n",
    "plt.plot(data_true[start_time:end_time], label='actual')\n",
    "plt.plot(mlp_predicted[start_time:end_time, 0], label=\"mlp\")\n",
    "plt.plot(lstm_predicted[start_time:end_time], label=\"lstm\")\n",
    "plt.plot(gru_predicted[start_time:end_time], label=\"gru\")\n",
    "plt.plot(cnn_predicted[start_time:end_time], label=\"cnn\")\n",
    "plt.plot(cnn_lstm_predicted[start_time:end_time, 0], label=\"cnn+gru\")\n",
    "plt.plot(lstnet_predicted[start_time:end_time, 0], label=\"lstnet\")\n",
    "plt.ylabel('Throughput (Mbits/sec)')\n",
    "plt.xlabel('Day')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_rmse = sqrt(mean_squared_error(data_true, mlp_predicted[:, 0]))\n",
    "mlp_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_rmse = sqrt(mean_squared_error(data_true, lstm_predicted))\n",
    "lstm_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_rmse = sqrt(mean_squared_error(data_true, gru_predicted))\n",
    "gru_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_rmse = sqrt(mean_squared_error(data_true, cnn_predicted))\n",
    "cnn_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_rmse = sqrt(mean_squared_error(data_true, cnn_lstm_predicted[:, 0]))\n",
    "cnn_lstm_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstnet_rmse = sqrt(mean_squared_error(data_true, lstnet_predicted[:, 0]))\n",
    "lstnet_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax=fig.add_axes([0,0,1,1])\n",
    "ax.bar(x=['ARIMA', 'MLP', 'LSTM', 'GRU', 'CNN', 'CNN+LSTM', 'LSTNet'], \n",
    "        height=[0.45, mlp_rmse, lstm_rmse, gru_rmse, cnn_rmse, cnn_lstm_rmse, lstnet_rmse])\n",
    "ax.set_xlabel('Model')\n",
    "\n",
    "ax.set_ylabel('Root Mean Squared Error (RMSE)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "  return np.mean(np.abs(y_true - y_pred) / np.abs(y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_mape = mean_absolute_percentage_error(data_true, mlp_predicted[:, 0])\n",
    "mlp_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mape = mean_absolute_percentage_error(data_true, lstm_predicted)\n",
    "lstm_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_mape = mean_absolute_percentage_error(data_true, gru_predicted)\n",
    "gru_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_mape = mean_absolute_percentage_error(data_true, cnn_predicted)\n",
    "cnn_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_mape = mean_absolute_percentage_error(data_true, cnn_lstm_predicted[:, 0])\n",
    "cnn_lstm_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstnet_mape = mean_absolute_percentage_error(data_true, lstnet_predicted[:, 0])\n",
    "lstnet_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax=fig.add_axes([0,0,1,1])\n",
    "ax.bar(x=['ARIMA', 'MLP', 'LSTM', 'GRU', 'CNN', 'CNN+LSTM', 'LSTNet'], \n",
    "        height=[79, mlp_mape, lstm_mape, gru_mape, cnn_mape, cnn_lstm_mape, lstnet_mape])\n",
    "ax.set_xlabel('Model')\n",
    "# ax.set_ylim(0.0, 0.9)\n",
    "ax.set_ylabel('Mean Absolute Percentage Error (MAPE) %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_corr = corr(data_true, mlp_predicted[:, 0])\n",
    "mlp_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_corr = corr(data_true, lstm_predicted)\n",
    "lstm_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_corr = corr(data_true, gru_predicted)\n",
    "gru_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_corr = corr(data_true, cnn_predicted)\n",
    "cnn_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_corr = corr(data_true, cnn_lstm_predicted[:, 0])\n",
    "cnn_lstm_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstnet_corr = corr(data_true, lstnet_predicted[:, 0])\n",
    "lstnet_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "scipy.stats.pearsonr(data_true, lstnet_predicted[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax=fig.add_axes([0,0,1,1])\n",
    "ax.bar(x=['ARIMA', 'MLP', 'LSTM', 'GRU', 'CNN', 'CNN+LSTM', 'LSTNet'], \n",
    "        height=[0.88, mlp_corr, lstm_corr, gru_corr, cnn_corr, cnn_lstm_corr, lstnet_corr])\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylim(0.85, 0.95)\n",
    "ax.set_ylabel('Mean Absolute Percentage Error (MAPE) %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_r2 = r2_score(data_true, mlp_predicted[:, 0])\n",
    "lstm_r2 = r2_score(data_true, lstm_predicted)\n",
    "gru_r2 = r2_score(data_true, gru_predicted)\n",
    "cnn_r2 = r2_score(data_true, cnn_predicted)\n",
    "cnn_lstm_r2 = r2_score(data_true, cnn_lstm_predicted[:, 0])\n",
    "lstnet_r2 = r2_score(data_true, lstnet_predicted[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'mlp r2:     {mlp_r2}')\n",
    "print(f'lstm r2:    {lstm_r2}')\n",
    "print(f'gru r2:     {gru_r2}')\n",
    "print(f'cnn r2:     {cnn_r2}')\n",
    "print(f'cnn+gru r2: {cnn_lstm_r2}')\n",
    "print(f'lstnet r2:  {lstnet_r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RRSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_rrse = rse(data_true, mlp_predicted[:, 0])\n",
    "mlp_rrse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_rrse = rse(data_true, lstm_predicted)\n",
    "lstm_rrse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_rrse = rse(data_true, gru_predicted)\n",
    "gru_rrse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_rrse = rse(data_true, cnn_predicted)\n",
    "cnn_rrse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_rrse = rse(data_true, cnn_lstm_predicted[:, 0].flatten())\n",
    "cnn_lstm_rrse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstnet_rrse = rse(data_true, lstnet_predicted[:, 0].flatten())\n",
    "lstnet_rrse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax=fig.add_axes([0,0,1,1])\n",
    "ax.bar(x=['ARIMA', 'MLP', 'LSTM', 'GRU', 'CNN', 'CNN+LSTM', 'LSTNet'], \n",
    "        height=[0.79, mlp_rrse,lstm_rrse,gru_rrse,cnn_rrse,cnn_lstm_rrse,lstnet_rrse])\n",
    "ax.set_xlabel('Model')\n",
    "# ax.set_ylim(0.0, 0.9)\n",
    "ax.set_ylabel('Root Relative Squared Error (RRSE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_mae = mean_absolute_error(data_true, mlp_predicted[:, 0])\n",
    "mlp_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mae = mean_absolute_error(data_true, lstm_predicted)\n",
    "lstm_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_mae = mean_absolute_error(data_true, gru_predicted)\n",
    "gru_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_mae = mean_absolute_error(data_true, cnn_predicted)\n",
    "cnn_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_mae = mean_absolute_error(data_true, cnn_lstm_predicted[:, 0])\n",
    "cnn_lstm_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstnet_mae = mean_absolute_error(data_true, lstnet_predicted[:, 0])\n",
    "lstnet_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax=fig.add_axes([0,0,1,1])\n",
    "ax.bar(x=['ARIMA', 'MLP', 'LSTM', 'GRU', 'CNN', 'CNN+LSTM', 'LSTNet'], \n",
    "        height=[79, mlp_mae, lstm_mae, gru_mae, cnn_mae, cnn_lstm_mae, lstnet_mae])\n",
    "ax.set_xlabel('Model')\n",
    "# ax.set_ylim(0.0, 0.9)\n",
    "ax.set_ylabel('Mean Absolute Percentage Error (MAPE) %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSRE (root mean square relative error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_mae = mean_absolute_error(data_true, mlp_predicted[:, 0])\n",
    "mlp_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mae = mean_absolute_error(data_true, lstm_predicted)\n",
    "lstm_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_mae = mean_absolute_error(data_true, gru_predicted)\n",
    "gru_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_mae = mean_absolute_error(data_true, cnn_predicted)\n",
    "cnn_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_mae = mean_absolute_error(data_true, cnn_lstm_predicted[:, 0])\n",
    "cnn_lstm_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstnet_mae = mean_absolute_error(data_true, lstnet_predicted[:, 0])\n",
    "lstnet_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax=fig.add_axes([0,0,1,1])\n",
    "ax.bar(x=['ARIMA', 'MLP', 'LSTM', 'GRU', 'CNN', 'CNN+LSTM', 'LSTNet'], \n",
    "        height=[79, mlp_mae, lstm_mae, gru_mae, cnn_mae, cnn_lstm_mae, lstnet_mae])\n",
    "ax.set_xlabel('Model')\n",
    "# ax.set_ylim(0.0, 0.9)\n",
    "ax.set_ylabel('Mean Absolute Percentage Error (MAPE) %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDF of Relative Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_relative_error = (data_true - mlp_predicted[:, 0]) / data_true\n",
    "lstm_relative_error = (data_true - lstm_predicted) / data_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_residual_error = (data_true - mlp_predicted[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_relative_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mlp_relative_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 40\n",
    "counts, bin_edges = np.histogram (abs(mlp_relative_error), bins=num_bins, normed=True)\n",
    "cdf = np.cumsum (counts)\n",
    "plt.plot (bin_edges[1:], cdf/cdf[-1])\n",
    "counts, bin_edges = np.histogram (abs(lstm_relative_error), bins=num_bins, normed=True)\n",
    "cdf = np.cumsum (counts)\n",
    "plt.plot (bin_edges[1:], cdf/cdf[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_actual = np.load('predicted_results/arima_data_actual_multivar_1.npy')\n",
    "arima_predicted = np.load('predicted_results/arima_data_predictions_multivar_1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(arima_actual[0:300])\n",
    "plt.plot(arima_predicted[0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_rrse = rse(arima_actual, arima_predicted)\n",
    "arima_rrse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "arima_mae = mean_absolute_error(arima_actual, arima_predicted)\n",
    "arima_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "  return np.mean(np.abs(y_true - y_pred) / np.abs(y_true)) * 100\n",
    "\n",
    "arima_mape = mean_absolute_percentage_error(arima_actual, arima_predicted)\n",
    "arima_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "arima_rmse = sqrt(mean_squared_error(arima_actual, arima_predicted))\n",
    "arima_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_corr = corr(arima_actual, arima_predicted)\n",
    "arima_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction for all models on a single route (from test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = np.load('numpy_group_test_data/1_stream_test_sample_group_95.npy')\n",
    "test_target = np.load('numpy_group_test_data/1_stream_test_target_group_95.npy')\n",
    "\n",
    "normalize_mean = 9726.925995055048\n",
    "normalize_std  = 6670.752950995383\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm \n",
    "\n",
    "arma_mod = sm.tsa.ARIMA(test_target[:,0], order=(8,0,8))\n",
    "arma_res = arma_mod.fit(trend='nc', disp=-1)\n",
    "arma_predictions = arma_res.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arma_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model instance\n",
    "\n",
    "# Restore the weights\n",
    "linear.load_weights('./checkpoints/linear_weights')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_test_predict = linear.predict(test_sample)\n",
    "lstm_test_predict = lstm_model.predict(test_sample)\n",
    "gru_test_predict = gru_model.predict(test_sample)\n",
    "cnn_test_predict = cnn_model.predict(test_sample)\n",
    "cnn_lstm_test_predict = cnn_lstm_model.predict(test_sample)\n",
    "lstnet_test_predict = lstnet_model.predict(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,7)\n",
    "plt.plot(test_target[1:,0] * normalize_std + normalize_mean, label='actual')\n",
    "plt.plot(arma_predictions[1:] * normalize_std + normalize_mean, label='arima')\n",
    "plt.plot(linear_test_predict[1:] * normalize_std + normalize_mean, label='mlp')\n",
    "plt.plot(lstm_test_predict[1:] * normalize_std + normalize_mean, label='lstm')\n",
    "plt.plot(gru_test_predict[1:] * normalize_std + normalize_mean, label='gru')\n",
    "plt.plot(cnn_test_predict[1:] * normalize_std + normalize_mean, label='cnn')\n",
    "plt.plot(cnn_lstm_test_predict[1:,0] * normalize_std + normalize_mean, label='cnn+gru')\n",
    "plt.plot(lstnet_test_predict[1:,0] * normalize_std + normalize_mean, label='lstnet')\n",
    "plt.ylabel('Throughput (Mbits/sec)')\n",
    "plt.xlabel('Day')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(test_target[1:,0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
